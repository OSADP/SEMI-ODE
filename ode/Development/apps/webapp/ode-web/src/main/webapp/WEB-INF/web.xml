<?xml version="1.0" encoding="ISO-8859-1"?>
<web-app xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
   xmlns="http://java.sun.com/xml/ns/javaee"
   xsi:schemaLocation="http://java.sun.com/xml/ns/javaee http://java.sun.com/xml/ns/javaee/web-app_2_5.xsd"
   id="WebApp_ID" version="2.5">
   <display-name>ode</display-name>

   <context-param>
      <param-name>contextConfigLocation</param-name>
      <param-value>classpath:spring/application-config.xml</param-value>
   </context-param>

   <context-param>
      <!-- The root context for ODE (NOT USED)-->
      <param-name>web.server.root</param-name>
      <param-value>ode</param-value>
   </context-param>
   <context-param>
      <!-- The root context for Liferay portal -->
      <param-name>liferay.ws.serverhost</param-name>
      <param-value>http://localhost:8080</param-value>
   </context-param>
   <context-param>
      <!-- Company ID of the Liferay portal instance. Will vary based on Destination installation -->
      <param-name>liferay.ws.companyId</param-name>
      <param-value>10157</param-value> 
   </context-param>
   <context-param>
      <!-- Liferay Portal database name -->
      <param-name>liferay.db.name</param-name>
      <param-value>lportal</param-value>
   </context-param>
   <context-param>
      <!-- Liferay Portal database host name -->
      <param-name>liferay.db.host</param-name>
      <param-value>localhost</param-value>
   </context-param>
   <context-param>
      <!-- smtp mail server host name (NOT USED)-->
      <param-name>mail.smtp.host</param-name>
      <param-value></param-value>
   </context-param>
   <context-param>
      <!-- smtp mail server port number (NOT USED)-->
      <param-name>mail.smtp.port</param-name>
      <param-value>25</param-value>
   </context-param>
   <context-param>
      <!-- smtp mail server socket factory port number (NOT USED)-->
      <param-name>mail.smtp.socketFactory.port</param-name>
      <param-value>25</param-value>
   </context-param>
   <context-param>
      <!-- FROM email address used for notifications (NOT USED)-->
      <param-name>mail.from</param-name>
      <param-value></param-value>
   </context-param>
   <context-param>
      <!-- smtp mail server username (NOT USED)-->
      <param-name>mail.user</param-name>
      <param-value></param-value>
   </context-param>
   <context-param>
      <!-- smtp mail server password (NOT USED)-->
      <param-name>mail.password</param-name>
      <param-value></param-value>
   </context-param>
   <context-param>
      <!-- USDOT CAS URL used for authenticating requests -->
      <param-name>dds.cas.url</param-name>
      <param-value>https://cas.connectedvcs.com/accounts/v1/tickets</param-value>
   </context-param>
   <context-param>
      <!-- USDOT CAS username used for authenticating requests -->
      <param-name>dds.cas.username</param-name>
      <param-value>${dds.cas.username}</param-value>
   </context-param>
   <context-param>
      <!-- USDOT CAS password used for authenticating requests -->
      <param-name>dds.cas.password</param-name>
      <param-value>${dds.cas.password}</param-value>
   </context-param>
   <context-param>
      <!-- USDOT Warehouse Tools WebSocket URI -->
      <param-name>dds.websocket.uri</param-name>
      <param-value>wss://webapp.connectedvcs.com/whtools22/websocket</param-value>
   </context-param>
   <context-param>
      <!-- Spark Master specifies if spark job is to run locally with K worker
           threads (local[k]) or on yarn (yarn-client or yarn-cluster). 
           If on yarn, whether to deploy your driver on the worker nodes (yarn-cluster)
           or locally as an external client (yarn-client) (default: client)   
      -->
      <param-name>spark.master</param-name>
      <param-value>yarn-client</param-value>
   </context-param>
   <context-param>
      <!-- Spark microbatch duration in milliseconds -->
      <param-name>spark.streaming.microbatch.duration.ms</param-name>
      <param-value>1000</param-value>
   </context-param>
   <context-param>
      <!-- Spark streaming sliding window duration in number of microbatches.
           multiply spark.streaming.window.microbatches by 
           spark.streaming.microbatch.duration.ms to get the sliding window
           duration in milliseconds.
       -->
      <param-name>spark.streaming.window.microbatches</param-name>
      <param-value>60</param-value>
   </context-param>
   <context-param>
      <!-- Spark streaming sliding window slide duration in number of microbatches.
           multiply spark.streaming.slide.microbatches by 
           spark.streaming.microbatch.duration.ms to get the slide duration 
           in milliseconds.
       -->
      <param-name>spark.streaming.slide.microbatches</param-name>
      <param-value>30</param-value>
   </context-param>
   
   <context-param>
      <!-- Weather integration file  
         CAN BE HDFS OR LOCAL 
         Local example: /home/centos/semi-weather.csv</param-value
         HDFS example: hdfs://ip-10-0-16-10.ec2.internal:8020/user/hdfs/semi-weather.csv
      -->
      <param-name>spark.static.weather.file.location</param-name>
      <param-value>hdfs://ip-10-0-16-10.ec2.internal:8020/user/hdfs/semi-weather.csv</param-value>
   </context-param>
   <context-param>
      <!-- Sanitization file  
         CAN BE HDFS OR LOCAL 
         Local example: /home/centos/sanitization.json</param-value
         HDFS example: hdfs://ip-10-0-16-10.ec2.internal:8020/user/hdfs/sanitization.json
      -->
      <param-name>spark.static.sanitization.file.location</param-name>
      <param-value>hdfs://ip-10-0-16-10.ec2.internal:8020/user/hdfs/sanitization.json</param-value>
   </context-param>
   <context-param>
      <!-- Validation file  
         CAN BE HDFS OR LOCAL 
         Local example: /home/centos/validation.json</param-value 
         HDFS example: hdfs://ip-10-0-16-10.ec2.internal:8020/user/hdfs/validation.json
      -->
      <param-name>spark.static.validation.file.location</param-name>
      <param-value>hdfs://ip-10-0-16-10.ec2.internal:8020/user/hdfs/validation.json</param-value>
   </context-param>
   <context-param>
      <param-name>spark.road.segment.snapping.tolerance</param-name>
      <param-value>20</param-value>
   </context-param>


   <context-param>
      <!-- Spark Assembly to be uploaded to Yarn Cluster Assumed to be installed 
         on the Web application Server This param specifies the name based on HDP 
         naming convention
      -->
      <param-name>spark.assembly.jar</param-name>
      <param-value>spark-assembly-1.3.1.2.3.0.0-2557-hadoop2.7.1.2.3.0.0-2557.jar</param-value>
   </context-param>

   <context-param>
      <!-- List of Kafka broker server host names ad port numbers-->
      <param-name>kafka.metadata.broker.list</param-name>
      <param-value>ip-10-0-16-115.ec2.internal:6667,ip-10-0-16-143.ec2.internal:6667,ip-10-0-16-10.ec2.internal:6667</param-value>
   </context-param>
   <context-param>
      <param-name>data.sequence.reorder.delay</param-name>
      <param-value>1000</param-value>
   </context-param>
   <context-param>
      <param-name>data.sequence.reorder.period</param-name>
      <param-value>1000</param-value>
   </context-param>
   <context-param>
      <!-- List of Zookeeper server host names and port numbers-->
      <param-name>zk.connection.strings</param-name>
      <param-value>ip-10-0-16-115.ec2.internal:2181,ip-10-0-16-10.ec2.internal:2181,ip-10-0-16-143.ec2.internal:2181</param-value>
   </context-param>
   <context-param>
      <!-- jar file name containing Spark ODE applications -->
      <param-name>ode.spark.jar</param-name>
      <param-value>ode-spark-0.0.1-SNAPSHOT.jar</param-value>
   </context-param>
   <context-param>
      <!-- RSA Keystore file used in ODE authentication -->
      <param-name>token.key.rsa.pem</param-name>
      <param-value>mykey.pem</param-value>
   </context-param>

   <!-- Configuration files and directory for Spark jobs. Currently two jobs
      are launched by the ODE: Transformer job and Aggregator job/ 
      Param value can be empty to disable the inclusion of external properties 
      files. Must be located within Servelet context, such as in /WEB-INF only 
      used in yarn-client or yarn-master modes 
   -->
   <context-param>
      <param-name>spark.yarn.transformer.configuration.file</param-name>
      <param-value>spark-yarn-transformer.properties</param-value>
   </context-param>
   <context-param>
      <param-name>spark.yarn.aggregator.configuration.file</param-name>
      <param-value>spark-yarn-aggregator.properties</param-value>
   </context-param>
   <context-param>
      <param-name>spark.configuration.file.directory</param-name>
      <param-value>/WEB-INF</param-value>
   </context-param>


   <!-- 
      Properties Value set in the Parent POM since the values are used 
      in more than one file 
   -->
   <context-param>
      <param-name>spark.metrics.transformer.configuration.file</param-name>
      <param-value>${spark.metrics.transformer.file}</param-value>
   </context-param>
   <context-param>
      <param-name>spark.metrics.aggregator.configuration.file</param-name>
      <param-value>${spark.metrics.aggregator.file}</param-value>
   </context-param>
   
   <!-- Metrics parameters -->
   <context-param>
      <param-name>metrics.prefix</param-name>
      <param-value>ode-</param-value>
   </context-param>
   <context-param>
      <param-name>metrics.polling.rate.seconds</param-name>
      <param-value>10</param-value>
   </context-param>
   <context-param>
      <param-name>metrics.graphite.host</param-name>
      <param-value>ip-10-0-8-41.ec2.internal</param-value>
   </context-param>
   <context-param>
      <param-name>metrics.graphite.port</param-name>
      <param-value>2003</param-value>
   </context-param>

   <!-- *** PERFORMANCE PARAMS *** -->
   <context-param>
      <param-name>dds.num.vsr.in.bundle.to.use</param-name>
      <param-value>10</param-value>
   </context-param>
   <context-param>
      <param-name>spark.yarn.transformer.driver.cores</param-name>
      <param-value>1</param-value>
   </context-param>
   <context-param>
      <param-name>spark.yarn.transformer.driver.memory</param-name>
      <param-value>512m</param-value>
   </context-param>
   <context-param>
      <param-name>spark.yarn.transformer.executor.cores</param-name>
      <param-value>2</param-value>
   </context-param>
   <context-param>
      <param-name>spark.yarn.transformer.executor.memory</param-name>
      <param-value>9216m</param-value>
   </context-param>

   <context-param>
      <param-name>spark.yarn.aggregator.driver.cores</param-name>
      <param-value>1</param-value>
   </context-param>
   <context-param>
      <param-name>spark.yarn.aggregator.driver.memory</param-name>
      <param-value>512m</param-value>
   </context-param>
   <context-param>
      <param-name>spark.yarn.aggregator.executor.cores</param-name>
      <param-value>1</param-value>
   </context-param>
   <context-param>
      <param-name>spark.yarn.aggregator.executor.memory</param-name>
      <param-value>2048m</param-value>
   </context-param>

   
   <!-- This param can be used to disable ODE Aggregator. Default = true -->
   <context-param>
      <param-name>spark.ode.aggregator.enabled</param-name>
      <param-value>true</param-value>
   </context-param>
   <!-- 
      This param can be used to run the aggregator in a sliding windnow 
      of size: spark.streaming.window.microbatches * spark.streaming.microbatch.duration.ms 
      OR 
      run it in a microbatch size of: 
      spark.streaming.window.microbatches * spark.streaming.microbatch.duration.ms
      
      Default: true
   -->
   <context-param>
      <param-name>spark.run.aggregator.in.sliding.window</param-name>
      <param-value>false</param-value>
   </context-param>
   <!-- 
      This param can be used to run the aggregator in the same Spark 
      application as the Vehicle Data Transformer application.
      Default: false
   -->
   <context-param>
      <param-name>spark.run.ode.aggregator.in.vdp</param-name>
      <param-value>false</param-value>
   </context-param>
   <!-- Number of kafka consumer threads -->
   <context-param>
      <param-name>kafka.consumer.threads</param-name>
      <param-value>9</param-value>
   </context-param>
   <!--
      This param can be used to change the spark scheduler mode of operation.
      Valid values are FIFO and FAIR. Default = FIFO
   -->
   <context-param>
      <param-name>spark.scheduler.mode</param-name>
      <param-value>FAIR</param-value>
   </context-param>
   <!--
      This param can be used to run the spark apps with a single kafka receiver
      supporting multiple topic partitions or route each kafka partition to 
      a separate Spark receiver. Default = false
      
      When false, one receiver is used to receive all partitions. When true,
      multiple receivers are dispatched to worker nodes to receive data in 
      parallel streams. the streams are then unified by spark before processing.
      This param should probably be set to true if a single receiver
      is the cause of data bottleneck.    
   -->
   <context-param>
      <param-name>spark.use.parallel.receivers</param-name>
      <param-value>false</param-value>
   </context-param>

   <listener>
      <listener-class>org.springframework.web.context.ContextLoaderListener</listener-class>
   </listener>
   <listener>
      <listener-class>com.bah.ode.server.InitServeletListener</listener-class>
   </listener>

   <servlet>
      <servlet-name>dispatcherServlet</servlet-name>
      <servlet-class>org.springframework.web.servlet.DispatcherServlet</servlet-class>
      <init-param>
         <param-name>contextConfigLocation</param-name>
         <param-value>/WEB-INF/mvc-config.xml</param-value>
      </init-param>
      <load-on-startup>1</load-on-startup>
   </servlet>
   <servlet-mapping>
      <servlet-name>dispatcherServlet</servlet-name>
      <url-pattern>/web/*</url-pattern>
   </servlet-mapping>
   <!-- <servlet> <description>InitServlet is intended to perform eager initializations 
      for ODE application.</description> <display-name>InitServlet</display-name> 
      <servlet-name>InitServlet</servlet-name> <servlet-class>com.bah.ode.server.InitServlet</servlet-class> 
      <load-on-startup>0</load-on-startup> </servlet> <servlet-mapping> <servlet-name>InitServlet</servlet-name> 
      <url-pattern>/api/sample/javax/*</url-pattern> </servlet-mapping> -->
   <servlet>
      <servlet-name>RESTfulServices</servlet-name>
      <servlet-class>org.glassfish.jersey.servlet.ServletContainer</servlet-class>
      <init-param>
         <param-name>javax.ws.rs.Application</param-name>
         <param-value>com.bah.ode.api.OdeResourceConfig</param-value>
      </init-param>

      <init-param>
         <param-name>jersey.config.server.provider.packages</param-name>
         <param-value>com.bah.ode.api</param-value>
      </init-param>
      <init-param>
         <!-- Default value is true for Jersery -->
         <param-name>jersey.config.server.provider.scanning.recursive</param-name>
         <param-value>true</param-value>
      </init-param>
      <load-on-startup>1</load-on-startup>
   </servlet>

   <servlet-mapping>
      <servlet-name>RESTfulServices</servlet-name>
      <url-pattern>/api/*</url-pattern>
   </servlet-mapping>

   <servlet>
      <servlet-name>ViewStatusMessages</servlet-name>
      <servlet-class>ch.qos.logback.classic.ViewStatusMessagesServlet</servlet-class>
   </servlet>

   <servlet-mapping>
      <servlet-name>ViewStatusMessages</servlet-name>
      <url-pattern>/lbClassicStatus</url-pattern>
   </servlet-mapping>
</web-app>
