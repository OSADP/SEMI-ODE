# Spark Standlone Properties
#

#spark.shuffle.service.enabled true
#spark.shuffle.manager sort
spark.scheduler.mode FIFO
spark.serializer org.apache.spark.serializer.KryoSerializer
spark.kafka.produce.type async

spark.streaming.microbatch.duration.ms 1000
spark.streaming.window.microbatches 60
spark.streaming.slide.microbatches 30
spark.static.weather.file.location hdfs://ip-10-0-16-10.ec2.internal:8020/user/root/semi-weather.csv
spark.static.sanitization.file.location hdfs://ip-10-0-16-10.ec2.internal:8020/user/root/sanitization.json
spark.static.validation.file.location hdfs://ip-10-0-16-10.ec2.internal:8020/user/root/validation.json
spark.vehicle.transformer.input.topic spark.vehicle.transformer.input.topic
spark.vehicle.aggregator.input.topic spark.vehicle.aggregator.input.topic
spark.vehicle.aggregator.output.topic spark.vehicle.aggregator.output.topic
spark.road.segment.snapping.tolerance 20
spark.receiver single
spark.ode.aggregator.enabled true
spark.run.aggregator.in.sliding.window true
spark.serializer org.apache.spark.serializer.KryoSerializer

#spark.driver.extraClassPath /usr/hdp/current/spark-client/lib/*:/usr/hdp/current/hadoop-client/*:/usr/hdp/current/hadoop-client/lib/*:/usr/hdp/current/hadoop-hdfs-client/*:/usr/hdp/current/hadoop-hdfs-client/lib/*
#spark.executor.extraClassPath /usr/hdp/current/spark-client/lib/*:/usr/hdp/current/hadoop-client/*:/usr/hdp/current/hadoop-client/lib/*:/usr/hdp/current/hadoop-hdfs-client/*:/usr/hdp/current/hadoop-hdfs-client/lib/*
#spark.executor.extraJavaOptions -agentlib:jdwp=transport=dt_socket,server=n,address=73.87.26.165:5005,suspend=n

# In milliseconds
spark.locality.wait 2000
spark.streaming.blockInterval 500

#logging parameters
spark.executor.logs.rolling.strategy size
spark.executor.logs.rolling.maxSize 64MB
spark.executor.logs.rolling.maxRetainedFiles 10

	#Enable periodic cleanup of worker / application directories. Applications directories are cleaned up regardless of whether the application is still running.
spark.worker.cleanup.enabled true
#(30 minutes)	Controls the interval, in seconds, at which the worker cleans up old application work dirs on the local machine.
spark.worker.cleanup.interval	1800
# (7 days)	The number of seconds to retain application work directories on each worker. This is a Time To Live and should depend on the amount of available disk space you have. Application logs and jars are downloaded to each application work dir. Over time, the work dirs can quickly fill up disk space, especially if you run jobs very frequently.
spark.worker.cleanup.appDataTtl	604800
