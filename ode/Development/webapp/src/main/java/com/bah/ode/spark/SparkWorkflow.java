package com.bah.ode.spark;

import java.util.Collections;
import java.util.Iterator;
import java.util.concurrent.atomic.AtomicInteger;

import org.apache.spark.api.java.JavaPairRDD;
import org.apache.spark.api.java.function.Function;
import org.apache.spark.api.java.function.VoidFunction;
import org.apache.spark.streaming.api.java.JavaPairReceiverInputDStream;
import org.apache.spark.streaming.api.java.JavaStreamingContext;
import org.apache.spark.streaming.kafka.KafkaUtils;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import scala.Tuple2;

import com.bah.ode.context.AppContext;
import com.bah.ode.wrapper.MQTopic;

public class SparkWorkflow {
   private static Logger logger = LoggerFactory
         .getLogger(SparkWorkflow.class);
   
   private AppContext appContext;
   private static AtomicInteger idCounter = new AtomicInteger();
   
   private String id;

   public SparkWorkflow(AppContext appContext) {
      super();
      this.appContext = appContext;
      this.id = SparkWorkflow.class.getName() + "-" + idCounter.getAndIncrement();
   }



   public void start(final MQTopic outboundTopic) {
      // Create a input stream with the custom receiver on target ip:port
      // and count the
      // words in input stream of \n delimited text (eg. generated by
      // 'nc')
      JavaStreamingContext ssc = appContext.getSparkStreamingConext();
      
      JavaPairReceiverInputDStream<String, String> kafkaStream = 
            KafkaUtils.createStream(
                  ssc, appContext.getParam(AppContext.ZK_CONNECTION_STRINGS),
                  String.valueOf(id), Collections.singletonMap(
                        outboundTopic.getName(), outboundTopic.getPartitions()));
      
      Function<JavaPairRDD<String, String>, Void> f1 = 
            new Function<JavaPairRDD<String, String>, Void>() {
         private static final long serialVersionUID = 4946021213014494671L;

         @Override
         public Void call(JavaPairRDD<String, String> rdd) throws Exception {
            VoidFunction<Iterator<Tuple2<String, String>>> f2 = new MQOutputer(
                  appContext.getParam(
                        AppContext.METADATA_BROKER_LIST),
                        outboundTopic);

            rdd.foreachPartition(f2);
            return null;
         }
      };

      kafkaStream.foreachRDD(f1);
      kafkaStream.print();
      ssc.start();
      logger.info("Workflow {} started", id);
   }

}
